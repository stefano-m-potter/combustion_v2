{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d15728-62fe-4eb8-9534-374d503066a8",
   "metadata": {},
   "source": [
    "above ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04323356-6970-436c-9b78-867a514d14ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load and clean data\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-08-13_LC_FISL_Original_combustionModelPredictors.csv\")\n",
    "\n",
    "out_path = \"/explore/nobackup/people/spotter5/new_combustion/all_data\"\n",
    "os.makedirs(out_path, exist_ok = True)\n",
    "\n",
    "# 2. Exclude columns not relevant for modeling\n",
    "exclude_columns = [\n",
    "    #'below.ground.carbon.combusted',\n",
    "    'above.carbon.combusted'\n",
    "    'burn.depth',\n",
    "    'burn_year',\n",
    "    #'rdnbr_old',\n",
    "    'project.name',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Date',\n",
    "    'id',\n",
    "    'CNA_MAR'\n",
    "    #  'fireYr',\n",
    "    # 'lat',\n",
    "    # 'lon',\n",
    "    # 'project_name'\n",
    "]\n",
    "\n",
    "# 3. Drop excluded columns and NaNs\n",
    "all_data = df.drop(columns=exclude_columns).dropna()\n",
    "\n",
    "\n",
    "# 1. Load and clean data\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-08-08_LC_FISL_Original_combustionModelPredictors.csv\")\n",
    "\n",
    "old = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/all_predictors.csv\")\n",
    "\n",
    "# Get the unique IDs to remove\n",
    "old_ids = old['id'].unique()\n",
    "\n",
    "# Filter the DataFrame using the ~ operator ✅\n",
    "new_data = df[~df['id'].isin(old_ids)].drop(columns=exclude_columns).dropna()\n",
    "\n",
    "# 1. Load and clean data\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-08-08_LC_FISL_Original_combustionModelPredictors.csv\")\n",
    "\n",
    "old = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/all_predictors.csv\")\n",
    "\n",
    "# Get the unique IDs to remove\n",
    "old_ids = old['id'].unique()\n",
    "\n",
    "# Filter the DataFrame using the ~ operator ✅\n",
    "old_data = df[df['id'].isin(old_ids)].drop(columns=exclude_columns).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f465b533-6859-43fa-839a-eed505c8c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'below.ground.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'below.ground.carbon.combusted'...\n",
      "  - Training on 'All Data' (1877 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.434\n",
      "  - Training on 'Old Data' (1010 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.228\n",
      "  - Training on 'New Data' (867 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.607\n",
      "\n",
      "Step 3: Generating 78 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: elevation\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: HLI\n",
      "  - Plotting for: TRASP\n",
      "  - Plotting for: aspect_rad\n",
      "  - Plotting for: slope_rad\n",
      "  - Plotting for: ruggedness\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'below.ground.carbon.combusted'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_belowground\n",
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'above.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'above.carbon.combusted'...\n",
      "  - Training on 'All Data' (1877 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.421\n",
      "  - Training on 'Old Data' (1010 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.339\n",
      "  - Training on 'New Data' (867 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.411\n",
      "\n",
      "Step 3: Generating 78 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: elevation\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: HLI\n",
      "  - Plotting for: TRASP\n",
      "  - Plotting for: aspect_rad\n",
      "  - Plotting for: slope_rad\n",
      "  - Plotting for: ruggedness\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'above.carbon.combusted'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_aboveground\n",
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'burn.depth'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'burn.depth'...\n",
      "  - Training on 'All Data' (1877 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.690\n",
      "  - Training on 'Old Data' (1010 rows)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 78)) while a minimum of 1 is required by RandomForestRegressor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Execute each analysis\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target_col, out_dir \u001b[38;5;129;01min\u001b[39;00m analyses\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mrun_pdp_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_CSV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mold_ids_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOLD_PREDICTORS_CSV\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉 All tasks finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m, in \u001b[0;36mrun_pdp_analysis\u001b[0;34m(target_variable, output_directory, input_csv_path, old_ids_csv_path)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m     71\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, oob_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    ...Done. Model OOB Score (R²): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf\u001b[38;5;241m.\u001b[39moob_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Store the trained model and predictor data\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/utils/validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1091\u001b[0m         )\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 78)) while a minimum of 1 is required by RandomForestRegressor."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def run_pdp_analysis(target_variable, output_directory, input_csv_path, old_ids_csv_path):\n",
    "    \"\"\"\n",
    "    Runs the full modeling and PDP generation pipeline for a specific target variable.\n",
    "\n",
    "    Args:\n",
    "        target_variable (str): The name of the column to use as the target.\n",
    "        output_directory (str): The path to save the generated PDP images.\n",
    "        input_csv_path (str): Path to the main input CSV file.\n",
    "        old_ids_csv_path (str): Path to the CSV containing old plot IDs.\n",
    "    \"\"\"\n",
    "    # --- Setup & Introduction ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 STARTING ANALYSIS | TARGET: '{target_variable}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # --- 1. Data Preparation ---\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    try:\n",
    "        df_main = pd.read_csv(input_csv_path)\n",
    "        df_old_ids = pd.read_csv(old_ids_csv_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error: Could not find input file. {e}\")\n",
    "        return\n",
    "\n",
    "    # Define columns to exclude from predictors.\n",
    "    # CRITICAL: We must exclude BOTH potential target variables from the predictors\n",
    "    # to prevent data leakage between the two analyses.\n",
    "    POTENTIAL_TARGETS = ['below.ground.carbon.combusted', 'above.carbon.combusted', 'burn.depth']\n",
    "    METADATA_COLUMNS = [\n",
    "        'burn_year', 'project.name', 'latitude', 'longitude', 'Date', 'id',\n",
    "        'CNA_MAR'\n",
    "    ]\n",
    "    \n",
    "    # Combine all columns to be dropped when creating the predictor set (X)\n",
    "    COLS_TO_DROP_FROM_X = POTENTIAL_TARGETS + METADATA_COLUMNS\n",
    "\n",
    "    # Prepare the three datasets (All, Old, New)\n",
    "    old_ids = df_old_ids['id'].unique()\n",
    "    data_splits = {\n",
    "        \"All Data\": df_main,\n",
    "        \"Old Data\": df_main[df_main['id'].isin(old_ids)],\n",
    "        \"New Data\": df_main[~df_main['id'].isin(old_ids)]\n",
    "    }\n",
    "\n",
    "    # --- 2. Model Training ---\n",
    "    print(f\"Step 2: Training Random Forest models on '{target_variable}'...\")\n",
    "    models = {}\n",
    "    for name, data in data_splits.items():\n",
    "        print(f\"  - Training on '{name}' ({len(data)} rows)...\")\n",
    "\n",
    "        # Drop rows where the CURRENT target variable is missing\n",
    "        df_clean = data.dropna(subset=[target_variable])\n",
    "\n",
    "        # Define predictors (X) and target (y)\n",
    "        X = df_clean.drop(columns=COLS_TO_DROP_FROM_X, errors='ignore')\n",
    "        y = df_clean[target_variable]\n",
    "\n",
    "        # Initialize and train the model\n",
    "        rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1, oob_score=True)\n",
    "        rf.fit(X, y)\n",
    "        print(f\"    ...Done. Model OOB Score (R²): {rf.oob_score_:.3f}\")\n",
    "\n",
    "        # Store the trained model and predictor data\n",
    "        models[name] = {'model': rf, 'X': X}\n",
    "\n",
    "    # --- 3. Generate and Save Partial Dependence Plots ---\n",
    "    feature_list = models['All Data']['X'].columns\n",
    "    print(f\"\\nStep 3: Generating {len(feature_list)} Partial Dependence Plots...\")\n",
    "\n",
    "    for feature in feature_list:\n",
    "        print(f\"  - Plotting for: {feature}\")\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\n",
    "        fig.suptitle(f'Partial Dependence on: {feature}\\n(Target: {target_variable})', fontsize=16, y=0.96)\n",
    "\n",
    "        plot_order = [\"All Data\", \"Old Data\", \"New Data\"]\n",
    "        for i, model_name in enumerate(plot_order):\n",
    "            ax = axes[i]\n",
    "            model_info = models[model_name]\n",
    "            PartialDependenceDisplay.from_estimator(\n",
    "                estimator=model_info['model'], X=model_info['X'], features=[feature],\n",
    "                ax=ax, line_kw={\"color\": \"darkcyan\", \"linewidth\": 2.5}\n",
    "            )\n",
    "            ax.set_title(f\"{model_name} (n={len(model_info['X'])})\")\n",
    "            ax.set_ylabel(\"Partial Dependence\")\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        save_path = os.path.join(output_directory, f'{feature}.png')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE for '{target_variable}'. Plots saved to: {output_directory}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Master Configuration ---\n",
    "    INPUT_CSV = \"/explore/nobackup/people/spotter5/new_combustion/2025-08-13_LC_FISL_Original_combustionModelPredictors.csv\"\n",
    "    # OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/all_predictors.csv\"\n",
    "    OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/Combustion_SynthesisData_05042018_XJW.csv\"\n",
    "    BASE_OUT_PATH = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "\n",
    "    # Define the analyses to run\n",
    "    analyses = {\n",
    "        'below.ground.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_belowground\"),\n",
    "        'above.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_aboveground\"),\n",
    "        'burn.depth': os.path.join(BASE_OUT_PATH, \"pdp_depth\")\n",
    "    }\n",
    "\n",
    "    # Execute each analysis\n",
    "    for target_col, out_dir in analyses.items():\n",
    "        run_pdp_analysis(\n",
    "            target_variable=target_col,\n",
    "            output_directory=out_dir,\n",
    "            input_csv_path=INPUT_CSV,\n",
    "            old_ids_csv_path=OLD_PREDICTORS_CSV\n",
    "        )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"🎉 All tasks finished.\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101ec83-4516-4525-a0f3-eb62319bb17e",
   "metadata": {},
   "source": [
    "With depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d75b8fb-a039-4d1e-8cd6-bcc7f61945b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'below.ground.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'below.ground.carbon.combusted'...\n",
      "  - Training on 'All Data' (1877 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.434\n",
      "  - Training on 'Old Data' (1010 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.226\n",
      "  - Training on 'New Data' (867 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.607\n",
      "\n",
      "Step 3: Generating 78 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: elevation\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: HLI\n",
      "  - Plotting for: TRASP\n",
      "  - Plotting for: aspect_rad\n",
      "  - Plotting for: slope_rad\n",
      "  - Plotting for: ruggedness\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'below.ground.carbon.combusted'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_belowground\n",
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'above.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'above.carbon.combusted'...\n",
      "  - Training on 'All Data' (1877 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.421\n",
      "  - Training on 'Old Data' (1010 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.342\n",
      "  - Training on 'New Data' (867 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.411\n",
      "\n",
      "Step 3: Generating 78 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: elevation\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: HLI\n",
      "  - Plotting for: TRASP\n",
      "  - Plotting for: aspect_rad\n",
      "  - Plotting for: slope_rad\n",
      "  - Plotting for: ruggedness\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'above.carbon.combusted'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_aboveground\n",
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'burn.depth'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "  - Backfilling 'burn.depth' from old CSV where missing in main...\n",
      "    Filled 904 missing burn.depth values.\n",
      "Step 2: Training Random Forest models on 'burn.depth'...\n",
      "  - Training on 'All Data' (1877 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.625\n",
      "  - Training on 'Old Data' (1010 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.502\n",
      "  - Training on 'New Data' (867 rows)...\n",
      "    ...Done. Model OOB Score (R²): 0.690\n",
      "\n",
      "Step 3: Generating 78 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: elevation\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: HLI\n",
      "  - Plotting for: TRASP\n",
      "  - Plotting for: aspect_rad\n",
      "  - Plotting for: slope_rad\n",
      "  - Plotting for: ruggedness\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'burn.depth'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_depth\n",
      "\n",
      "======================================================================\n",
      "🎉 All tasks finished.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def run_pdp_analysis(target_variable, output_directory, input_csv_path, old_ids_csv_path):\n",
    "    \"\"\"\n",
    "    Runs the full modeling and PDP generation pipeline for a specific target variable.\n",
    "\n",
    "    Args:\n",
    "        target_variable (str): The name of the column to use as the target.\n",
    "        output_directory (str): The path to save the generated PDP images.\n",
    "        input_csv_path (str): Path to the main input CSV file.\n",
    "        old_ids_csv_path (str): Path to the CSV containing old plot IDs and, if available, burn.depth.\n",
    "    \"\"\"\n",
    "    # --- Setup & Introduction ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 STARTING ANALYSIS | TARGET: '{target_variable}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # --- 1. Data Preparation ---\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    try:\n",
    "        df_main = pd.read_csv(input_csv_path)\n",
    "        df_old_ids = pd.read_csv(old_ids_csv_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error: Could not find input file. {e}\")\n",
    "        return\n",
    "\n",
    "    # Normalize 'id' types & dedupe\n",
    "    if 'id' not in df_main.columns or 'id' not in df_old_ids.columns:\n",
    "        print(\"❌ Error: Both input files must have an 'id' column.\")\n",
    "        return\n",
    "\n",
    "    for df in (df_main, df_old_ids):\n",
    "        # coerce to numeric if possible (keeps strings if not)\n",
    "        try:\n",
    "            df['id'] = pd.to_numeric(df['id'], errors='ignore')\n",
    "        except Exception:\n",
    "            pass\n",
    "    df_old_ids = df_old_ids.drop_duplicates(subset='id')\n",
    "\n",
    "    # If we're modeling burn.depth, try to backfill from old file (if present there)\n",
    "    if target_variable == 'burn.depth' and 'burn.depth' in df_old_ids.columns:\n",
    "        print(\"  - Backfilling 'burn.depth' from old CSV where missing in main...\")\n",
    "        before_na = df_main['burn.depth'].isna().sum() if 'burn.depth' in df_main.columns else None\n",
    "        if 'burn.depth' not in df_main.columns:\n",
    "            df_main['burn.depth'] = np.nan\n",
    "        df_main = df_main.set_index('id')\n",
    "        df_old_depth = df_old_ids.set_index('id')['burn.depth']\n",
    "        df_main['burn.depth'] = df_main['burn.depth'].combine_first(df_old_depth)\n",
    "        df_main = df_main.reset_index()\n",
    "        after_na = df_main['burn.depth'].isna().sum()\n",
    "        if before_na is not None:\n",
    "            print(f\"    Filled {before_na - after_na} missing burn.depth values.\")\n",
    "\n",
    "    # Define columns to exclude from predictors (X)\n",
    "    POTENTIAL_TARGETS = ['below.ground.carbon.combusted', 'above.carbon.combusted', 'burn.depth']\n",
    "    METADATA_COLUMNS = [\n",
    "        'burn_year', 'project.name', 'latitude', 'longitude', 'Date', 'id', 'CNA_MAR'\n",
    "    ]\n",
    "    COLS_TO_DROP_FROM_X = POTENTIAL_TARGETS + METADATA_COLUMNS\n",
    "\n",
    "    # Prepare splits\n",
    "    old_ids = df_old_ids['id'].unique()\n",
    "    data_splits = {\n",
    "        \"All Data\": df_main,\n",
    "        \"Old Data\": df_main[df_main['id'].isin(old_ids)],\n",
    "        \"New Data\": df_main[~df_main['id'].isin(old_ids)]\n",
    "    }\n",
    "\n",
    "    # --- 2. Model Training ---\n",
    "    print(f\"Step 2: Training Random Forest models on '{target_variable}'...\")\n",
    "    models = {}\n",
    "    for name, data in data_splits.items():\n",
    "        print(f\"  - Training on '{name}' ({len(data)} rows)...\")\n",
    "\n",
    "        if target_variable not in data.columns:\n",
    "            print(f\"    ⚠️ Skipping '{name}' – target '{target_variable}' not in columns.\")\n",
    "            continue\n",
    "\n",
    "        # Drop rows missing the CURRENT target variable\n",
    "        df_clean = data.dropna(subset=[target_variable]).copy()\n",
    "\n",
    "        # Build X (numeric only), drop constant/all-NaN cols\n",
    "        X = df_clean.drop(columns=COLS_TO_DROP_FROM_X, errors='ignore')\n",
    "        X = X.select_dtypes(include=[np.number])\n",
    "        if X.shape[1] == 0:\n",
    "            print(f\"    ⚠️ Skipping '{name}' – no numeric predictors after cleaning.\")\n",
    "            continue\n",
    "        # drop all-NaN columns\n",
    "        X = X.loc[:, X.notna().any(axis=0)]\n",
    "        # drop constant columns\n",
    "        constant_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "        if constant_cols:\n",
    "            X = X.drop(columns=constant_cols)\n",
    "\n",
    "        y = df_clean[target_variable].astype(float)\n",
    "\n",
    "        n = len(y)\n",
    "        if n < 2:\n",
    "            print(f\"    ⚠️ Skipping '{name}' – insufficient samples after dropna (n={n}).\")\n",
    "            continue\n",
    "\n",
    "        # Enable OOB only when there are enough samples to make it meaningful\n",
    "        use_oob = n > 10\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            oob_score=use_oob\n",
    "        )\n",
    "        rf.fit(X, y)\n",
    "        if use_oob:\n",
    "            print(f\"    ...Done. Model OOB Score (R²): {rf.oob_score_:.3f}\")\n",
    "        else:\n",
    "            print(f\"    ...Done. (OOB disabled; n={n})\")\n",
    "\n",
    "        models[name] = {'model': rf, 'X': X}\n",
    "\n",
    "    if \"All Data\" not in models:\n",
    "        print(\"❌ No trainable 'All Data' model. Aborting PDP stage.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Generate and Save Partial Dependence Plots ---\n",
    "    feature_list = models['All Data']['X'].columns\n",
    "    print(f\"\\nStep 3: Generating {len(feature_list)} Partial Dependence Plots...\")\n",
    "\n",
    "    # Only include splits that successfully trained\n",
    "    trained_order = [k for k in [\"All Data\", \"Old Data\", \"New Data\"] if k in models]\n",
    "    if not trained_order:\n",
    "        print(\"❌ No trained models available for PDP.\")\n",
    "        return\n",
    "\n",
    "    for feature in feature_list:\n",
    "        print(f\"  - Plotting for: {feature}\")\n",
    "        fig, axes = plt.subplots(len(trained_order), 1, figsize=(8, 4 * len(trained_order)), sharex=True)\n",
    "        if len(trained_order) == 1:\n",
    "            axes = [axes]\n",
    "        fig.suptitle(f'Partial Dependence on: {feature}\\n(Target: {target_variable})', fontsize=16, y=0.96)\n",
    "\n",
    "        for ax, model_name in zip(axes, trained_order):\n",
    "            model_info = models[model_name]\n",
    "            try:\n",
    "                PartialDependenceDisplay.from_estimator(\n",
    "                    estimator=model_info['model'], X=model_info['X'], features=[feature],\n",
    "                    ax=ax, line_kw={\"color\": \"darkcyan\", \"linewidth\": 2.5}\n",
    "                )\n",
    "                ax.set_title(f\"{model_name} (n={len(model_info['X'])})\")\n",
    "                ax.set_ylabel(\"Partial Dependence\")\n",
    "                ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            except Exception as e:\n",
    "                ax.set_title(f\"{model_name} – PDP failed for '{feature}' ({e})\")\n",
    "                ax.axis('off')\n",
    "\n",
    "        save_path = os.path.join(output_directory, f'{feature}.png')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE for '{target_variable}'. Plots saved to: {output_directory}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Master Configuration ---\n",
    "    INPUT_CSV = \"/explore/nobackup/people/spotter5/new_combustion/2025-08-13_LC_FISL_Original_combustionModelPredictors.csv\"\n",
    "    OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/Combustion_SynthesisData_05042018_XJW.csv\"\n",
    "    BASE_OUT_PATH = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "\n",
    "    analyses = {\n",
    "        'below.ground.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_belowground\"),\n",
    "        'above.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_aboveground\"),\n",
    "        'burn.depth': os.path.join(BASE_OUT_PATH, \"pdp_depth\")\n",
    "    }\n",
    "\n",
    "    for target_col, out_dir in analyses.items():\n",
    "        run_pdp_analysis(\n",
    "            target_variable=target_col,\n",
    "            output_directory=out_dir,\n",
    "            input_csv_path=INPUT_CSV,\n",
    "            old_ids_csv_path=OLD_PREDICTORS_CSV\n",
    "        )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"🎉 All tasks finished.\")\n",
    "    print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceaa4973-6195-4a06-80bc-546c2f433216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'below.ground.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'below.ground.carbon.combusted'...\n",
      "  - Preparing data for 'All Data' (1877 rows)...\n",
      "    ... Cleaned predictor NaNs. Dropped 562 rows. Final training size: 1201\n",
      "    ... Done. Model OOB Score (R²): 0.358\n",
      "  - Preparing data for 'Old Data' (1011 rows)...\n",
      "    ... Cleaned predictor NaNs. Dropped 127 rows. Final training size: 770\n",
      "    ... Done. Model OOB Score (R²): 0.218\n",
      "  - Preparing data for 'New Data' (866 rows)...\n",
      "    ... Cleaned predictor NaNs. Dropped 435 rows. Final training size: 431\n",
      "    ... Done. Model OOB Score (R²): 0.559\n",
      "\n",
      "Step 3: Generating 73 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: slope_rad\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'below.ground.carbon.combusted'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_belowground\n",
      "\n",
      "======================================================================\n",
      "🚀 STARTING ANALYSIS | TARGET: 'above.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'above.carbon.combusted'...\n",
      "  - Preparing data for 'All Data' (1877 rows)...\n",
      "    ... Cleaned predictor NaNs. Dropped 501 rows. Final training size: 980\n",
      "    ... Done. Model OOB Score (R²): 0.355\n",
      "  - Preparing data for 'Old Data' (1011 rows)...\n",
      "    ... Cleaned predictor NaNs. Dropped 66 rows. Final training size: 549\n",
      "    ... Done. Model OOB Score (R²): 0.341\n",
      "  - Preparing data for 'New Data' (866 rows)...\n",
      "    ... Cleaned predictor NaNs. Dropped 435 rows. Final training size: 431\n",
      "    ... Done. Model OOB Score (R²): 0.330\n",
      "\n",
      "Step 3: Generating 73 Partial Dependence Plots...\n",
      "  - Plotting for: PFI\n",
      "  - Plotting for: pH_30\n",
      "  - Plotting for: Sand_30\n",
      "  - Plotting for: Silt_30\n",
      "  - Plotting for: Clay_30\n",
      "  - Plotting for: DOB_lst\n",
      "  - Plotting for: Relative.humidity\n",
      "  - Plotting for: Temperature\n",
      "  - Plotting for: VPD\n",
      "  - Plotting for: Wind.speed\n",
      "  - Plotting for: JP\n",
      "  - Plotting for: BS\n",
      "  - Plotting for: DEC\n",
      "  - Plotting for: GRSH\n",
      "  - Plotting for: NV\n",
      "  - Plotting for: OCON\n",
      "  - Plotting for: WS\n",
      "  - Plotting for: CNA_Tmax_5_8\n",
      "  - Plotting for: CNA_PPT_5_8\n",
      "  - Plotting for: CNA_Rad_5_8\n",
      "  - Plotting for: CNA_DD_0_5_8\n",
      "  - Plotting for: CNA_DD5_5_8\n",
      "  - Plotting for: CNA_DD_18_5_8\n",
      "  - Plotting for: CNA_DD18_sm\n",
      "  - Plotting for: CNA_NFFD_5_8\n",
      "  - Plotting for: CNA_PAS_5_8\n",
      "  - Plotting for: CNA_Eref_5_8\n",
      "  - Plotting for: CNA_CMD_5_8\n",
      "  - Plotting for: CNA_RH_5_8\n",
      "  - Plotting for: CNA_Tave_5_8\n",
      "  - Plotting for: CNA_Tmin_5_8\n",
      "  - Plotting for: CNA_MAT\n",
      "  - Plotting for: CNA_MWMT\n",
      "  - Plotting for: CNA_MCMT\n",
      "  - Plotting for: CNA_TD\n",
      "  - Plotting for: CNA_MAP\n",
      "  - Plotting for: CNA_MSP\n",
      "  - Plotting for: CNA_AHM\n",
      "  - Plotting for: CNA_SHM\n",
      "  - Plotting for: CNA_DD_0\n",
      "  - Plotting for: CNA_DD5\n",
      "  - Plotting for: CNA_DD_18\n",
      "  - Plotting for: CNA_DD18\n",
      "  - Plotting for: CNA_NFFD\n",
      "  - Plotting for: CNA_bFFP\n",
      "  - Plotting for: CNA_eFFP\n",
      "  - Plotting for: CNA_FFP\n",
      "  - Plotting for: CNA_PAS\n",
      "  - Plotting for: CNA_EMT\n",
      "  - Plotting for: CNA_EXT\n",
      "  - Plotting for: CNA_Eref\n",
      "  - Plotting for: CNA_CMD\n",
      "  - Plotting for: CNA_RH\n",
      "  - Plotting for: BD_30\n",
      "  - Plotting for: NDII\n",
      "  - Plotting for: NDVI\n",
      "  - Plotting for: SOC_30\n",
      "  - Plotting for: brightness\n",
      "  - Plotting for: dNBR\n",
      "  - Plotting for: greenness\n",
      "  - Plotting for: rbr\n",
      "  - Plotting for: Tree.cover\n",
      "  - Plotting for: wetness\n",
      "  - Plotting for: rdnbr\n",
      "  - Plotting for: BUI\n",
      "  - Plotting for: DC\n",
      "  - Plotting for: DMC\n",
      "  - Plotting for: FFMC\n",
      "  - Plotting for: FWI\n",
      "  - Plotting for: ISI\n",
      "  - Plotting for: DSR\n",
      "  - Plotting for: twi\n",
      "  - Plotting for: slope_rad\n",
      "\n",
      "✅ ANALYSIS COMPLETE for 'above.carbon.combusted'. Plots saved to: /explore/nobackup/people/spotter5/new_combustion/pdp_aboveground\n",
      "\n",
      "======================================================================\n",
      "🎉 All tasks finished.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def run_pdp_analysis(target_variable, output_directory, input_csv_path, old_ids_csv_path):\n",
    "    \"\"\"\n",
    "    Runs the full modeling and PDP generation pipeline for a specific target variable.\n",
    "\n",
    "    Args:\n",
    "        target_variable (str): The name of the column to use as the target.\n",
    "        output_directory (str): The path to save the generated PDP images.\n",
    "        input_csv_path (str): Path to the main input CSV file.\n",
    "        old_ids_csv_path (str): Path to the CSV containing old plot IDs.\n",
    "    \"\"\"\n",
    "    # --- Setup & Introduction ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🚀 STARTING ANALYSIS | TARGET: '{target_variable}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # --- 1. Data Preparation ---\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    try:\n",
    "        df_main = pd.read_csv(input_csv_path)\n",
    "        df_old_ids = pd.read_csv(old_ids_csv_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error: Could not find input file. {e}\")\n",
    "        return\n",
    "\n",
    "    # Define columns to exclude from predictors.\n",
    "    POTENTIAL_TARGETS = ['below.ground.carbon.combusted', 'above.carbon.combusted']\n",
    "    METADATA_COLUMNS = [\n",
    "        'burn_year', 'project.name', 'latitude', 'longitude', 'Date', 'id',\n",
    "        'CNA_MAR', 'fireYr', 'lat', 'lon', 'project_name'\n",
    "    ]\n",
    "    \n",
    "    # Combine all columns to be dropped when creating the predictor set (X)\n",
    "    COLS_TO_DROP_FROM_X = POTENTIAL_TARGETS + METADATA_COLUMNS\n",
    "\n",
    "    # Prepare the three datasets (All, Old, New)\n",
    "    old_ids = df_old_ids['id'].unique()\n",
    "    data_splits = {\n",
    "        \"All Data\": df_main,\n",
    "        \"Old Data\": df_main[df_main['id'].isin(old_ids)],\n",
    "        \"New Data\": df_main[~df_main['id'].isin(old_ids)]\n",
    "    }\n",
    "\n",
    "    # --- 2. Model Training ---\n",
    "    print(f\"Step 2: Training Random Forest models on '{target_variable}'...\")\n",
    "    models = {}\n",
    "    for name, data in data_splits.items():\n",
    "        print(f\"  - Preparing data for '{name}' ({len(data)} rows)...\")\n",
    "\n",
    "        # First, drop rows where the CURRENT target variable is missing\n",
    "        df_clean_target = data.dropna(subset=[target_variable])\n",
    "\n",
    "        # Define predictors (X) and target (y) from this pre-cleaned data\n",
    "        X = df_clean_target.drop(columns=COLS_TO_DROP_FROM_X, errors='ignore')\n",
    "        y = df_clean_target[target_variable]\n",
    "\n",
    "        # --- START: FIX ---\n",
    "        # **CRITICAL FIX**: Now, drop rows with NaNs in the PREDICTOR (X) columns\n",
    "        # This ensures the model receives completely clean data.\n",
    "        rows_before_cleaning_predictors = len(X)\n",
    "        X = X.dropna()\n",
    "        \n",
    "        # **CRITICAL FIX**: Re-align y to match the cleaned X's index\n",
    "        y = y.loc[X.index]\n",
    "        rows_after_cleaning_predictors = len(X)\n",
    "        \n",
    "        if rows_before_cleaning_predictors > rows_after_cleaning_predictors:\n",
    "            rows_dropped = rows_before_cleaning_predictors - rows_after_cleaning_predictors\n",
    "            print(f\"    ... Cleaned predictor NaNs. Dropped {rows_dropped} rows. Final training size: {rows_after_cleaning_predictors}\")\n",
    "        # --- END: FIX ---\n",
    "\n",
    "        # Check if there is still data to train on\n",
    "        if X.empty:\n",
    "            print(f\"    ... ❌ Skipping '{name}': No data left after cleaning.\")\n",
    "            models[name] = None # Store None to indicate a failed model\n",
    "            continue\n",
    "\n",
    "        # Initialize and train the model\n",
    "        rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1, oob_score=True)\n",
    "        rf.fit(X, y)\n",
    "        print(f\"    ... Done. Model OOB Score (R²): {rf.oob_score_:.3f}\")\n",
    "\n",
    "        # Store the trained model and predictor data\n",
    "        models[name] = {'model': rf, 'X': X}\n",
    "\n",
    "    # --- 3. Generate and Save Partial Dependence Plots ---\n",
    "    # Use the 'All Data' model's features as the reference list\n",
    "    if models['All Data'] is None:\n",
    "        print(\"\\n❌ Cannot generate plots because 'All Data' model failed to train.\")\n",
    "        return\n",
    "        \n",
    "    feature_list = models['All Data']['X'].columns\n",
    "    print(f\"\\nStep 3: Generating {len(feature_list)} Partial Dependence Plots...\")\n",
    "\n",
    "    for feature in feature_list:\n",
    "        print(f\"  - Plotting for: {feature}\")\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 12), sharex=True, squeeze=False) # squeeze=False ensures axes is always 2D\n",
    "        axes = axes.flatten() # Flatten to a 1D array for easy iteration\n",
    "        fig.suptitle(f'Partial Dependence on: {feature}\\n(Target: {target_variable})', fontsize=16, y=0.96)\n",
    "\n",
    "        plot_order = [\"All Data\", \"Old Data\", \"New Data\"]\n",
    "        for i, model_name in enumerate(plot_order):\n",
    "            ax = axes[i]\n",
    "            model_info = models.get(model_name) # Use .get() for safety\n",
    "\n",
    "            if model_info:\n",
    "                PartialDependenceDisplay.from_estimator(\n",
    "                    estimator=model_info['model'], X=model_info['X'], features=[feature],\n",
    "                    ax=ax, line_kw={\"color\": \"darkcyan\", \"linewidth\": 2.5}\n",
    "                )\n",
    "                ax.set_title(f\"{model_name} (n={len(model_info['X'])})\")\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Model could not be trained\\n(No data available)', \n",
    "                        ha='center', va='center', transform=ax.transAxes, fontsize=12, color='red')\n",
    "                ax.set_title(f\"{model_name} (n=0)\")\n",
    "\n",
    "            ax.set_ylabel(\"Partial Dependence\")\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        save_path = os.path.join(output_directory, f'{feature}.png')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE for '{target_variable}'. Plots saved to: {output_directory}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Master Configuration ---\n",
    "    INPUT_CSV = \"/explore/nobackup/people/spotter5/new_combustion/2025-08-08_LC_FISL_Original_combustionModelPredictors.csv\"\n",
    "    OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/all_predictors.csv\"\n",
    "    BASE_OUT_PATH = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "\n",
    "    # Define the analyses to run\n",
    "    analyses = {\n",
    "        'below.ground.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_belowground\"),\n",
    "        'above.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_aboveground\")\n",
    "    }\n",
    "\n",
    "    # Execute each analysis\n",
    "    for target_col, out_dir in analyses.items():\n",
    "        run_pdp_analysis(\n",
    "            target_variable=target_col,\n",
    "            output_directory=out_dir,\n",
    "            input_csv_path=INPUT_CSV,\n",
    "            old_ids_csv_path=OLD_PREDICTORS_CSV\n",
    "        )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"🎉 All tasks finished.\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2b902e-b167-46da-86b6-c1f7ff1c844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 STARTING PDP (New Data only)\n",
      "======================================================================\n",
      "New Data rows: 867\n",
      "\n",
      "— Target: above.carbon.combusted\n",
      "  ✓ Trained RF (n=867)\n",
      "\n",
      "— Target: below.ground.carbon.combusted\n",
      "  ✓ Trained RF (n=867)\n",
      "\n",
      "— Target: burn.depth\n",
      "  ✓ Trained RF (n=866)\n",
      "\n",
      "======================================================================\n",
      "🎉 Done. Histograms (sky-blue) + manual PDP lines saved in *_new_3x2 folders.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# -------------------- Font/Style: large text everywhere --------------------\n",
    "FS_SUPTITLE = 36   # figure title\n",
    "FS_LABEL    = 26   # x/y axis labels\n",
    "FS_TICKS    = 22   # tick labels\n",
    "FS_SUBTITLE = 26   # per-panel titles\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": FS_TICKS,\n",
    "    \"axes.labelsize\": FS_LABEL,\n",
    "    \"xtick.labelsize\": FS_TICKS,\n",
    "    \"ytick.labelsize\": FS_TICKS,\n",
    "    \"legend.fontsize\": FS_TICKS,\n",
    "})\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "INPUT_CSV = \"/explore/nobackup/people/spotter5/new_combustion/2025-08-13_LC_FISL_Original_combustionModelPredictors.csv\"\n",
    "OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/Combustion_SynthesisData_05042018_XJW.csv\"\n",
    "BASE_OUT_PATH = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "\n",
    "OUT_DIRS = {\n",
    "    'above.carbon.combusted':        os.path.join(BASE_OUT_PATH, \"pdp_aboveground_fisl\"),\n",
    "    'below.ground.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_belowground_fisl\"),\n",
    "    'burn.depth':                    os.path.join(BASE_OUT_PATH, \"pdp_depth_fisl\"),\n",
    "}\n",
    "\n",
    "POTENTIAL_TARGETS = ['below.ground.carbon.combusted', 'above.carbon.combusted', 'burn.depth']\n",
    "METADATA_COLUMNS = ['burn_year','project.name','latitude','longitude','Date','id','CNA_MAR',\n",
    "                    'fireYr','lat','lon','project_name']\n",
    "COLS_TO_DROP_FROM_X = POTENTIAL_TARGETS + METADATA_COLUMNS\n",
    "\n",
    "# Requested top-6 features\n",
    "TOP6 = {\n",
    "    'above.carbon.combusted':        ['BS','twi','brightness','ISI','Tree.cover','Temperature'],\n",
    "    'below.ground.carbon.combusted': ['BUI','twi','CNA_PAS','Silt_30','brightness','NV'],\n",
    "    'burn.depth':                    ['BUI','twi','CNA_PAS','CNA_DD_0','WS','DC'],\n",
    "}\n",
    "\n",
    "# Pretty labels for plotting\n",
    "PRETTY_LABELS = {\n",
    "    'BS':           'Black Spruce',\n",
    "    'twi':          'The Wetness Index',\n",
    "    'brightness':   'Brightness',\n",
    "    'ISI':          'Initial Spread Index',\n",
    "    'Tree.cover':   'Tree Cover',\n",
    "    'Temperature':  'Temperature (FWI)',\n",
    "    'BUI':          'Buildup Index',\n",
    "    'CNA_PAS':      'Precipitation as Snow',\n",
    "    'Silt_30':      'Silt %',\n",
    "    'NV':           'Non Vegetation',\n",
    "    'CNA_DD_0':     'Growing Degree Days < 0',\n",
    "    'WS':           'White Spruce',\n",
    "    'DC':           'Drought code',\n",
    "}\n",
    "\n",
    "# Histogram style\n",
    "HIST_BINS  = 40\n",
    "HIST_ALPHA = 0.30\n",
    "HIST_COLOR = 'skyblue'\n",
    "\n",
    "# Manual PDP config\n",
    "PDP_MAX_SAMPLES   = 5000   # average predictions over up to this many rows\n",
    "PDP_GRID_POINTS   = 60     # grid resolution along x\n",
    "PDP_CLIP_PERCENT  = (1, 99)  # clip to avoid extreme tails\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def normalize_name(s):\n",
    "    return \"\".join(ch for ch in str(s).lower() if ch.isalnum())\n",
    "\n",
    "def map_requested_features_to_columns_with_labels(requested, df_columns):\n",
    "    \"\"\"Return list of (actual_column_name, pretty_label) pairs.\"\"\"\n",
    "    norm_map = {normalize_name(c): c for c in df_columns}\n",
    "    mapped_pairs, missing = [], []\n",
    "    for r in requested:\n",
    "        key = normalize_name(r)\n",
    "        if key in norm_map:\n",
    "            mapped_pairs.append((norm_map[key], PRETTY_LABELS.get(r, r)))\n",
    "        else:\n",
    "            missing.append(r)\n",
    "    return mapped_pairs, missing\n",
    "\n",
    "def median_impute_numeric(df):\n",
    "    out = df.copy()\n",
    "    num_cols = out.select_dtypes(include=[np.number]).columns\n",
    "    out[num_cols] = out[num_cols].fillna(out[num_cols].median())\n",
    "    return out\n",
    "\n",
    "def manual_pdp(model, X: pd.DataFrame, feature: str,\n",
    "               max_samples=PDP_MAX_SAMPLES,\n",
    "               grid_points=PDP_GRID_POINTS,\n",
    "               clip_percent=PDP_CLIP_PERCENT,\n",
    "               random_state=42):\n",
    "    \"\"\"\n",
    "    Compute 1D partial dependence for `feature` by averaging predictions\n",
    "    over rows of X while sweeping `feature` across an evenly spaced grid\n",
    "    between the given percentiles.\n",
    "    \"\"\"\n",
    "    if feature not in X.columns:\n",
    "        return None, None\n",
    "\n",
    "    # sample rows for speed\n",
    "    X_base = X.sample(n=min(len(X), max_samples), random_state=random_state) if len(X) > max_samples else X\n",
    "\n",
    "    vals = X_base[feature].to_numpy()\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if vals.size < 2:\n",
    "        return None, None\n",
    "\n",
    "    lo, hi = np.nanpercentile(vals, clip_percent)\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:\n",
    "        return None, None\n",
    "\n",
    "    grid = np.linspace(lo, hi, grid_points)\n",
    "    pdp = np.empty_like(grid, dtype=float)\n",
    "\n",
    "    X_work = X_base.copy()\n",
    "    for i, g in enumerate(grid):\n",
    "        X_work[feature] = g\n",
    "        pdp[i] = model.predict(X_work).mean()\n",
    "\n",
    "    return grid, pdp\n",
    "\n",
    "# -------------------- Load and split --------------------\n",
    "print(f\"\\n{'='*70}\\n🚀 STARTING PDP (New Data only)\\n{'='*70}\")\n",
    "df_main = pd.read_csv(INPUT_CSV)\n",
    "df_old  = pd.read_csv(OLD_PREDICTORS_CSV)\n",
    "\n",
    "if 'id' not in df_main.columns or 'id' not in df_old.columns:\n",
    "    raise SystemExit(\"❌ Both files must have an 'id' column.\")\n",
    "\n",
    "old_ids = pd.unique(df_old['id'])\n",
    "df_new  = df_main[~df_main['id'].isin(old_ids)].copy()\n",
    "print(f\"New Data rows: {len(df_new)}\")\n",
    "\n",
    "# -------------------- Train & plot --------------------\n",
    "for target in OUT_DIRS:\n",
    "    os.makedirs(OUT_DIRS[target], exist_ok=True)\n",
    "    print(f\"\\n— Target: {target}\")\n",
    "\n",
    "    if target not in df_new.columns:\n",
    "        print(f\"  ⚠️ Target '{target}' not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_t = df_new.dropna(subset=[target]).copy()\n",
    "    if df_t.empty:\n",
    "        print(f\"  ⚠️ No rows with non-NA '{target}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    X = df_t.drop(columns=COLS_TO_DROP_FROM_X, errors='ignore').select_dtypes(include=[np.number])\n",
    "    if X.shape[1] == 0:\n",
    "        print(\"  ⚠️ No numeric predictors after exclusions. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    X_imp = median_impute_numeric(X)\n",
    "    y = df_t[target].astype(float)\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=len(y) > 10\n",
    "    )\n",
    "    rf.fit(X_imp, y)\n",
    "    print(f\"  ✓ Trained RF (n={len(y)})\")\n",
    "\n",
    "    requested = TOP6[target]\n",
    "    mapped_pairs, missing = map_requested_features_to_columns_with_labels(requested, X_imp.columns)\n",
    "    if not mapped_pairs:\n",
    "        print(\"  ⚠️ No requested features found. Skipping PDP.\")\n",
    "        continue\n",
    "\n",
    "    # figure setup with big fonts\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(22, 26))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(6):\n",
    "        ax = axes[i]\n",
    "        if i < len(mapped_pairs):\n",
    "            feat_col, pretty_label = mapped_pairs[i]\n",
    "\n",
    "            # --- Histogram (secondary y-axis, behind the line) ---\n",
    "            ax_hist = ax.twinx()\n",
    "            values = X_imp[feat_col].to_numpy()\n",
    "            values = values[np.isfinite(values)]\n",
    "            if values.size > 0:\n",
    "                ax_hist.hist(values, bins=HIST_BINS, alpha=HIST_ALPHA, color=HIST_COLOR)\n",
    "            ax_hist.set_yticks([])\n",
    "            ax_hist.set_ylabel(\"\")\n",
    "            ax_hist.tick_params(axis='both', length=0)\n",
    "            ax_hist.grid(False)\n",
    "            ax_hist.set_zorder(1)\n",
    "            ax.set_zorder(2)\n",
    "            ax.patch.set_visible(False)\n",
    "\n",
    "            # --- Manual PDP line (primary y-axis) ---\n",
    "            xs, ys = manual_pdp(rf, X_imp, feat_col)\n",
    "            if xs is not None:\n",
    "                ax.plot(xs, ys, linewidth=4, color=\"black\", zorder=3)\n",
    "                # align x-limits to data range so hist + line overlap cleanly\n",
    "                xmin, xmax = (np.min(values), np.max(values)) if values.size else (xs.min(), xs.max())\n",
    "                if np.isfinite(xmin) and np.isfinite(xmax) and xmin != xmax:\n",
    "                    ax.set_xlim(xmin, xmax)\n",
    "                    ax_hist.set_xlim(xmin, xmax)\n",
    "\n",
    "            # Labels/titles\n",
    "            ax.set_title(pretty_label, fontsize=FS_SUBTITLE, pad=18)\n",
    "            ax.set_xlabel(pretty_label, fontsize=FS_LABEL)\n",
    "            pretty_target = {\n",
    "                'above.carbon.combusted': 'Aboveground Carbon',\n",
    "                'below.ground.carbon.combusted': 'Belowground Carbon',\n",
    "                'burn.depth': 'Burn Depth'\n",
    "            }[target]\n",
    "            ax.set_ylabel(pretty_target, fontsize=FS_LABEL)\n",
    "            ax.tick_params(axis='both', labelsize=FS_TICKS)\n",
    "            ax.xaxis.labelpad = 14\n",
    "            ax.yaxis.labelpad = 14\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    pretty = {\n",
    "        'above.carbon.combusted': 'Aboveground Carbon',\n",
    "        'below.ground.carbon.combusted': 'Belowground Carbon',\n",
    "        'burn.depth': 'Burn Depth'\n",
    "    }[target]\n",
    "    fig.suptitle(f'New Data — Top 6 PDPs ({pretty})', fontsize=FS_SUPTITLE, y=0.98)\n",
    "\n",
    "    save_path = os.path.join(OUT_DIRS[target], f\"pdp_new_{target.replace('.', '_')}_3x2.png\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\n{'='*70}\\n🎉 Done. Histograms (sky-blue) + manual PDP lines saved in *_new_3x2 folders.\\n{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f80fee-3d64-455b-a068-f7b630fee506",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d5db2d-5463-4a16-9369-1e367eef1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 STARTING PDP (New Data only)\n",
      "======================================================================\n",
      "New Data rows: 867\n",
      "\n",
      "— Target: above.carbon.combusted\n",
      "  ✓ Trained RF (n=867)\n",
      "\n",
      "— Target: below.ground.carbon.combusted\n",
      "  ✓ Trained RF (n=867)\n",
      "\n",
      "— Target: burn.depth\n",
      "  ✓ Trained RF (n=866)\n",
      "\n",
      "======================================================================\n",
      "🎉 Done. PDP vectors pulled from sklearn + sky-blue histograms.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# -------------------- Font/Style: large text everywhere --------------------\n",
    "FS_SUPTITLE = 36   # figure title\n",
    "FS_LABEL    = 26   # x/y axis labels\n",
    "FS_TICKS    = 22   # tick labels\n",
    "FS_SUBTITLE = 26   # per-panel titles\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": FS_TICKS,\n",
    "    \"axes.labelsize\": FS_LABEL,\n",
    "    \"xtick.labelsize\": FS_TICKS,\n",
    "    \"ytick.labelsize\": FS_TICKS,\n",
    "    \"legend.fontsize\": FS_TICKS,\n",
    "})\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "INPUT_CSV = \"/explore/nobackup/people/spotter5/new_combustion/2025-08-13_LC_FISL_Original_combustionModelPredictors.csv\"\n",
    "OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/Combustion_SynthesisData_05042018_XJW.csv\"\n",
    "BASE_OUT_PATH = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "\n",
    "OUT_DIRS = {\n",
    "    'above.carbon.combusted':        os.path.join(BASE_OUT_PATH, \"pdp_aboveground_fisl\"),\n",
    "    'below.ground.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_belowground_fisl\"),\n",
    "    'burn.depth':                    os.path.join(BASE_OUT_PATH, \"pdp_depth_fisl\"),\n",
    "}\n",
    "\n",
    "POTENTIAL_TARGETS = ['below.ground.carbon.combusted', 'above.carbon.combusted', 'burn.depth']\n",
    "METADATA_COLUMNS = ['burn_year','project.name','latitude','longitude','Date','id','CNA_MAR',\n",
    "                    'fireYr','lat','lon','project_name']\n",
    "COLS_TO_DROP_FROM_X = POTENTIAL_TARGETS + METADATA_COLUMNS\n",
    "\n",
    "# Requested top-6 features\n",
    "TOP6 = {\n",
    "    'above.carbon.combusted':        ['BS','twi','brightness','ISI','Tree.cover','Temperature'],\n",
    "    'below.ground.carbon.combusted': ['BUI','twi','CNA_PAS','Silt_30','brightness','NV'],\n",
    "    'burn.depth':                    ['BUI','twi','CNA_PAS','CNA_DD_0','WS','DC'],\n",
    "}\n",
    "\n",
    "# Pretty labels for plotting\n",
    "PRETTY_LABELS = {\n",
    "    'BS':           'Black Spruce',\n",
    "    'twi':          'The Wetness Index',\n",
    "    'brightness':   'Brightness',\n",
    "    'ISI':          'Initial Spread Index',\n",
    "    'Tree.cover':   'Tree Cover',\n",
    "    'Temperature':  'Temperature (FWI)',\n",
    "    'BUI':          'Buildup Index',\n",
    "    'CNA_PAS':      'Precipitation as Snow',\n",
    "    'Silt_30':      'Silt %',\n",
    "    'NV':           'Non Vegetation',\n",
    "    'CNA_DD_0':     'Growing Degree Days < 0',\n",
    "    'WS':           'White Spruce',\n",
    "    'DC':           'Drought code',\n",
    "}\n",
    "\n",
    "# Histogram style\n",
    "HIST_BINS  = 40\n",
    "HIST_ALPHA = 0.30\n",
    "HIST_COLOR = 'skyblue'\n",
    "\n",
    "# PDP config\n",
    "PDP_GRID_RES = 60\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def normalize_name(s):\n",
    "    return \"\".join(ch for ch in str(s).lower() if ch.isalnum())\n",
    "\n",
    "def map_requested_features_to_columns_with_labels(requested, df_columns):\n",
    "    \"\"\"Return list of (actual_column_name, pretty_label) pairs.\"\"\"\n",
    "    norm_map = {normalize_name(c): c for c in df_columns}\n",
    "    mapped_pairs, missing = [], []\n",
    "    for r in requested:\n",
    "        key = normalize_name(r)\n",
    "        if key in norm_map:\n",
    "            mapped_pairs.append((norm_map[key], PRETTY_LABELS.get(r, r)))\n",
    "        else:\n",
    "            missing.append(r)\n",
    "    return mapped_pairs, missing\n",
    "\n",
    "def median_impute_numeric(df):\n",
    "    out = df.copy()\n",
    "    num_cols = out.select_dtypes(include=[np.number]).columns\n",
    "    out[num_cols] = out[num_cols].fillna(out[num_cols].median())\n",
    "    return out\n",
    "\n",
    "def pdp_curve_from_sklearn(estimator, X, feature, grid_resolution=PDP_GRID_RES):\n",
    "    \"\"\"\n",
    "    Try sklearn's partial_dependence to get (x, y) PDP arrays.\n",
    "    Fallback: call PartialDependenceDisplay.from_estimator on a temp axis and\n",
    "    extract line data, then close the temp figure.\n",
    "    \"\"\"\n",
    "    # Attempt modern API first\n",
    "    try:\n",
    "        from sklearn.inspection import partial_dependence\n",
    "        res = partial_dependence(\n",
    "            estimator, X, [feature], kind=\"average\",\n",
    "            grid_resolution=grid_resolution, method=\"auto\"\n",
    "        )\n",
    "        # Newer versions expose either 'values' or 'grid_values'\n",
    "        xs = res.get('values', res.get('grid_values'))[0]\n",
    "        ys = res['average'][0].ravel()\n",
    "        return np.asarray(xs), np.asarray(ys)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: render off-screen and grab the line data\n",
    "    fig_tmp, ax_tmp = plt.subplots()\n",
    "    try:\n",
    "        disp = PartialDependenceDisplay.from_estimator(\n",
    "            estimator, X, [feature], kind=\"average\",\n",
    "            grid_resolution=grid_resolution, ax=ax_tmp\n",
    "        )\n",
    "        # First (and only) line in first (and only) axis for 1D PDP\n",
    "        line = disp.lines_[0][0]\n",
    "        xs = np.asarray(line.get_xdata())\n",
    "        ys = np.asarray(line.get_ydata())\n",
    "        return xs, ys\n",
    "    finally:\n",
    "        plt.close(fig_tmp)\n",
    "\n",
    "# -------------------- Load and split --------------------\n",
    "print(f\"\\n{'='*70}\\n🚀 STARTING PDP (New Data only)\\n{'='*70}\")\n",
    "df_main = pd.read_csv(INPUT_CSV)\n",
    "df_old  = pd.read_csv(OLD_PREDICTORS_CSV)\n",
    "\n",
    "if 'id' not in df_main.columns or 'id' not in df_old.columns:\n",
    "    raise SystemExit(\"❌ Both files must have an 'id' column.\")\n",
    "\n",
    "old_ids = pd.unique(df_old['id'])\n",
    "df_new  = df_main[~df_main['id'].isin(old_ids)].copy()\n",
    "print(f\"New Data rows: {len(df_new)}\")\n",
    "\n",
    "# -------------------- Train & plot --------------------\n",
    "for target in OUT_DIRS:\n",
    "    os.makedirs(OUT_DIRS[target], exist_ok=True)\n",
    "    print(f\"\\n— Target: {target}\")\n",
    "\n",
    "    if target not in df_new.columns:\n",
    "        print(f\"  ⚠️ Target '{target}' not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_t = df_new.dropna(subset=[target]).copy()\n",
    "    if df_t.empty:\n",
    "        print(f\"  ⚠️ No rows with non-NA '{target}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    X = df_t.drop(columns=COLS_TO_DROP_FROM_X, errors='ignore').select_dtypes(include=[np.number])\n",
    "    if X.shape[1] == 0:\n",
    "        print(\"  ⚠️ No numeric predictors after exclusions. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    X_imp = median_impute_numeric(X)\n",
    "    y = df_t[target].astype(float)\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=len(y) > 10\n",
    "    )\n",
    "    rf.fit(X_imp, y)\n",
    "    print(f\"  ✓ Trained RF (n={len(y)})\")\n",
    "\n",
    "    requested = TOP6[target]\n",
    "    mapped_pairs, missing = map_requested_features_to_columns_with_labels(requested, X_imp.columns)\n",
    "    if not mapped_pairs:\n",
    "        print(\"  ⚠️ No requested features found. Skipping PDP.\")\n",
    "        continue\n",
    "\n",
    "    # figure setup with big fonts\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(22, 26))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(6):\n",
    "        ax = axes[i]\n",
    "        if i < len(mapped_pairs):\n",
    "            feat_col, pretty_label = mapped_pairs[i]\n",
    "\n",
    "            # --- Histogram (secondary y-axis, behind the line) ---\n",
    "            ax_hist = ax.twinx()\n",
    "            values = X_imp[feat_col].to_numpy()\n",
    "            values = values[np.isfinite(values)]\n",
    "            if values.size > 0:\n",
    "                ax_hist.hist(values, bins=HIST_BINS, alpha=HIST_ALPHA, color=HIST_COLOR)\n",
    "            ax_hist.set_yticks([])\n",
    "            ax_hist.set_ylabel(\"\")\n",
    "            ax_hist.tick_params(axis='both', length=0)\n",
    "            ax_hist.grid(False)\n",
    "            ax_hist.set_zorder(1)\n",
    "            ax.set_zorder(2)\n",
    "            ax.patch.set_visible(False)\n",
    "\n",
    "            # --- PDP line (extracted from sklearn, plotted manually) ---\n",
    "            xs, ys = pdp_curve_from_sklearn(rf, X_imp, feat_col, grid_resolution=PDP_GRID_RES)\n",
    "            if xs is not None and ys is not None:\n",
    "                ax.plot(xs, ys, linewidth=4, color=\"black\", zorder=3)\n",
    "                # Align x-limits for clean overlay\n",
    "                xmin, xmax = (np.min(values), np.max(values)) if values.size else (np.min(xs), np.max(xs))\n",
    "                if np.isfinite(xmin) and np.isfinite(xmax) and xmin != xmax:\n",
    "                    ax.set_xlim(xmin, xmax)\n",
    "                    ax_hist.set_xlim(xmin, xmax)\n",
    "\n",
    "            # Labels/titles\n",
    "            ax.set_title(pretty_label, fontsize=FS_SUBTITLE, pad=18)\n",
    "            ax.set_xlabel(pretty_label, fontsize=FS_LABEL)\n",
    "            pretty_target = {\n",
    "                'above.carbon.combusted': 'Aboveground Carbon',\n",
    "                'below.ground.carbon.combusted': 'Belowground Carbon',\n",
    "                'burn.depth': 'Burn Depth'\n",
    "            }[target]\n",
    "            ax.set_ylabel(pretty_target, fontsize=FS_LABEL)\n",
    "            ax.tick_params(axis='both', labelsize=FS_TICKS)\n",
    "            ax.xaxis.labelpad = 14\n",
    "            ax.yaxis.labelpad = 14\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    pretty = {\n",
    "        'above.carbon.combusted': 'Aboveground Carbon',\n",
    "        'below.ground.carbon.combusted': 'Belowground Carbon',\n",
    "        'burn.depth': 'Burn Depth'\n",
    "    }[target]\n",
    "    fig.suptitle(f'New Data — Top 6 PDPs ({pretty})', fontsize=FS_SUPTITLE, y=0.98)\n",
    "\n",
    "    save_path = os.path.join(OUT_DIRS[target], f\"pdp_new_{target.replace('.', '_')}_3x2.png\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\n{'='*70}\\n🎉 Done. PDP vectors pulled from sklearn + sky-blue histograms.\\n{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeabe53-603c-4813-8332-4ef82c94a809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
