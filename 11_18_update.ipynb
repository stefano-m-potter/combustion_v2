{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68097073-d89b-4b22-89c7-9d01a6e2fc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1281, 60)\n",
      "(100, 2)\n",
      "(98, 57)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "df1 = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-10-03_CombustionModelPredictors.csv\")\n",
    "df2 = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-11-18_LCC_burnDepthField.csv\")\n",
    "df2 = df2.rename(columns = {'burnDepthField': 'burn_depth')\n",
    "df3 = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-10-28_LCC_CombustionModelPredictors.csv\")\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e72cb685-0742-416c-b24d-d2f621c6e1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df1 shape: (1174, 60)\n",
      "New combined dataframe shape (after join/concat/dropna): (1225, 61)\n",
      "\n",
      "Target: burn_depth | X: (1225, 50) | y: (1225,)\n",
      "\n",
      "[burn_depth] Starting global RandomizedSearchCV with 40 iterations...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "[burn_depth] Best params: {'bootstrap': True, 'max_depth': 14, 'max_features': 0.40142583666029136, 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 312}\n",
      "[burn_depth] RandomizedSearch best CV RMSE: 4.2950 units\n",
      "\n",
      "[burn_depth] Starting LOOCV with fixed best hyperparameters...\n",
      "  LOOCV progress: 25/1225\n",
      "  LOOCV progress: 50/1225\n",
      "  LOOCV progress: 75/1225\n",
      "  LOOCV progress: 100/1225\n",
      "  LOOCV progress: 125/1225\n",
      "  LOOCV progress: 150/1225\n",
      "  LOOCV progress: 175/1225\n",
      "  LOOCV progress: 200/1225\n",
      "  LOOCV progress: 225/1225\n",
      "  LOOCV progress: 250/1225\n",
      "  LOOCV progress: 275/1225\n",
      "  LOOCV progress: 300/1225\n",
      "  LOOCV progress: 325/1225\n",
      "  LOOCV progress: 350/1225\n",
      "  LOOCV progress: 375/1225\n",
      "  LOOCV progress: 400/1225\n",
      "  LOOCV progress: 425/1225\n",
      "  LOOCV progress: 450/1225\n",
      "  LOOCV progress: 475/1225\n",
      "  LOOCV progress: 500/1225\n",
      "  LOOCV progress: 525/1225\n",
      "  LOOCV progress: 550/1225\n",
      "  LOOCV progress: 575/1225\n",
      "  LOOCV progress: 600/1225\n",
      "  LOOCV progress: 625/1225\n",
      "  LOOCV progress: 650/1225\n",
      "  LOOCV progress: 675/1225\n",
      "  LOOCV progress: 700/1225\n",
      "  LOOCV progress: 725/1225\n",
      "  LOOCV progress: 750/1225\n",
      "  LOOCV progress: 775/1225\n",
      "  LOOCV progress: 800/1225\n",
      "  LOOCV progress: 825/1225\n",
      "  LOOCV progress: 850/1225\n",
      "  LOOCV progress: 875/1225\n",
      "  LOOCV progress: 900/1225\n",
      "  LOOCV progress: 925/1225\n",
      "  LOOCV progress: 950/1225\n",
      "  LOOCV progress: 975/1225\n",
      "  LOOCV progress: 1000/1225\n",
      "  LOOCV progress: 1025/1225\n",
      "  LOOCV progress: 1050/1225\n",
      "  LOOCV progress: 1075/1225\n",
      "  LOOCV progress: 1100/1225\n",
      "  LOOCV progress: 1125/1225\n",
      "  LOOCV progress: 1150/1225\n",
      "  LOOCV progress: 1175/1225\n",
      "  LOOCV progress: 1200/1225\n",
      "  LOOCV progress: 1225/1225\n",
      "[burn_depth] LOOCV RMSE (global): 4.2373 | R² (global, clamped): 0.4968 (raw: 0.4968)\n",
      "[burn_depth] Saved per-fold R² CSV: /explore/nobackup/people/spotter5/new_combustion/LCC/burn_depth_violin_LCC_11_18.csv\n",
      "[burn_depth] Saved LOOCV predictions (with fold R²) to: /explore/nobackup/people/spotter5/new_combustion/LCC/burn_depth_loocv_predictions_LCC_11_18.csv\n",
      "[burn_depth] Saved LCC-only Obs vs Pred plot: /explore/nobackup/people/spotter5/new_combustion/LCC/burn_depth_loocv_obs_pred_LCC_only_LCC_11_18.png\n",
      "[burn_depth] Saved violin plot of per-fold R²: /explore/nobackup/people/spotter5/new_combustion/LCC/burn_depth_violin_LCC_11_18.png\n",
      "[burn_depth] Saved feature importance CSV: /explore/nobackup/people/spotter5/new_combustion/LCC/burn_depth_feature_importance_LCC_11_18.csv\n",
      "[burn_depth] Saved feature importance plot: /explore/nobackup/people/spotter5/new_combustion/LCC/burn_depth_feature_importance_LCC_11_18.png\n",
      "[burn_depth] Final model trained with 50 features; 'burn_depth' in features? False\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd  # not used yet, but kept since you imported it\n",
    "\n",
    "# ----------------- Load data -----------------\n",
    "df1 = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-10-03_CombustionModelPredictors.csv\").dropna(subset = 'burn_depth')\n",
    "\n",
    "df2 = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-11-18_LCC_burnDepthField.csv\")\n",
    "# fix syntax + rename to burn_depth\n",
    "df2 = df2.rename(columns={'burnDepthField': 'burn_depth'})\n",
    "\n",
    "df3 = pd.read_csv(\"/explore/nobackup/people/spotter5/new_combustion/2025-10-28_LCC_CombustionModelPredictors.csv\")\n",
    "\n",
    "# ----------------- Join LCC predictors with burn depth on ID -----------------\n",
    "# Inner join = only IDs that appear in BOTH df2 (burn depth) and df3 (LCC predictors)\n",
    "df_lcc = pd.merge(\n",
    "    df3,\n",
    "    df2[['ID', 'burn_depth']],  # keep only ID + burn_depth from df2\n",
    "    on='ID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# ----------------- Concatenate with main training file -----------------\n",
    "df_all = pd.concat([df1, df_lcc], ignore_index=True, sort=False)\n",
    "\n",
    "# ----------------- Drop rows with missing burn_depth -----------------\n",
    "df_all = df_all.dropna(subset=['burn_depth'])\n",
    "\n",
    "# ----------------- Print shapes -----------------\n",
    "print(\"Original df1 shape:\", df1.shape)\n",
    "print(\"New combined dataframe shape (after join/concat/dropna):\", df_all.shape)\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from scipy.stats import randint, uniform\n",
    "import joblib\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "# Assume df_all already exists in memory from your previous join/concat.\n",
    "# If not, you could recreate it here and then:\n",
    "# df = df_all.copy()\n",
    "\n",
    "OUT_DIR   = \"/explore/nobackup/people/spotter5/new_combustion/LCC\"\n",
    "MODEL_DIR = os.path.join(OUT_DIR, \"models\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Suffix for ALL outputs and models\n",
    "SUFFIX = \"_LCC_11_18\"\n",
    "\n",
    "# ----------------- SEARCH CONFIG -----------------\n",
    "RANDOM_STATE   = 42\n",
    "N_JOBS         = -1\n",
    "INNER_FOLDS    = 5\n",
    "N_ITER_SEARCH  = 40\n",
    "SCORER         = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "RF_PARAM_DIST = {\n",
    "    \"n_estimators\": randint(200, 1000),\n",
    "    \"max_depth\":    randint(3, 40),\n",
    "    \"max_features\": uniform(0.2, 0.8),  # float in (0,1]: fraction of features\n",
    "    \"min_samples_split\": randint(2, 20),\n",
    "    \"min_samples_leaf\":  randint(1, 10),\n",
    "    \"bootstrap\":   [True, False]\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# LOAD / PREP DATAFRAME\n",
    "# ============================================================\n",
    "# Use df_all from previous step\n",
    "df = df_all.copy()\n",
    "\n",
    "# ----------------- BASIC CLEANUP -----------------\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "rename_map = {}\n",
    "if 'ID' in df.columns: rename_map['ID'] = 'id'\n",
    "if 'Id' in df.columns: rename_map['Id'] = 'id'\n",
    "if 'project_name' in df.columns and 'project.name' not in df.columns:\n",
    "    rename_map['project_name'] = 'project.name'\n",
    "if 'Date' in df.columns and 'date' not in df.columns:\n",
    "    rename_map['Date'] = 'date'\n",
    "if 'latitude' in df.columns and 'lat' not in df.columns:\n",
    "    rename_map['latitude'] = 'lat'\n",
    "if 'longitude' in df.columns and 'lon' not in df.columns:\n",
    "    rename_map['longitude'] = 'lon'\n",
    "if 'fireYr' in df.columns and 'burn_year' not in df.columns:\n",
    "    rename_map['fireYr'] = 'burn_year'\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Schema snapshot\n",
    "schema = pd.DataFrame({\n",
    "    \"column\": df.columns,\n",
    "    \"dtype\": df.dtypes.astype(str),\n",
    "    \"n_null\": df.isna().sum(),\n",
    "    \"n_unique\": [df[c].nunique(dropna=True) for c in df.columns]\n",
    "})\n",
    "schema.to_csv(os.path.join(OUT_DIR, f\"schema_summary{SUFFIX}.csv\"), index=False)\n",
    "\n",
    "# ----------------- CATEGORICAL: LandCover -> one-hot -----------------\n",
    "if 'LandCover' in df.columns:\n",
    "    df = pd.get_dummies(df, columns=['LandCover'], prefix='LC', drop_first=True, dummy_na=False)\n",
    "\n",
    "# ----------------- EXCLUDED PREDICTOR COLUMNS -----------------\n",
    "EXCLUDE_PRED_COLS = {\n",
    "    'id', 'project.name', 'lat', 'lon', 'burn_year', 'date', 'project',\n",
    "    'ID', 'Id', 'project_name', 'latitude', 'longitude', 'fireYr', 'Date',\n",
    "    'landcover_name'\n",
    "}\n",
    "\n",
    "# ----------------- TARGET PICKER -----------------\n",
    "def pick_col(candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "COL_ABOVE = pick_col(['combusted_above', 'above.carbon.combusted'])\n",
    "COL_BELOW = pick_col(['combusted_below'])\n",
    "COL_DEPTH = pick_col(['burn_depth'])\n",
    "\n",
    "# >>> Train burn_depth only for now <<<\n",
    "targets = [(c, \"units\") for c in [COL_DEPTH] if c]\n",
    "if not targets:\n",
    "    raise ValueError(\"None of the expected target columns were found in the dataset.\")\n",
    "\n",
    "# IMPORTANT: include ALL possible targets for safe dropping\n",
    "ALL_TARGET_COLS = [c for c in [COL_ABOVE, COL_BELOW, COL_DEPTH] if c]\n",
    "\n",
    "# ============================================================\n",
    "# Helper: build X, y, ids (keep ID for plotting LCC-only later)\n",
    "# ============================================================\n",
    "def build_xy(df_in: pd.DataFrame, target_col: str):\n",
    "    # Find an ID column to keep (if present)\n",
    "    id_col = None\n",
    "    for c in ['id', 'ID', 'Id']:\n",
    "        if c in df_in.columns:\n",
    "            id_col = c\n",
    "            break\n",
    "\n",
    "    # Drop everything in EXCLUDE_PRED_COLS EXCEPT the id_col we want to keep\n",
    "    drop_cols = [c for c in EXCLUDE_PRED_COLS if (c in df_in.columns and c != id_col)]\n",
    "    work = df_in.drop(columns=drop_cols, errors='ignore').copy()\n",
    "    work = work.dropna(subset=[target_col])\n",
    "\n",
    "    y = work[target_col].astype(float).copy()\n",
    "\n",
    "    # Capture IDs before we strip non-numeric predictors\n",
    "    if id_col and id_col in work.columns:\n",
    "        ids = work[id_col].copy()\n",
    "    else:\n",
    "        ids = pd.Series(range(len(work)), index=work.index, name='id')\n",
    "\n",
    "    # Drop the explicit target AND any other known target columns to prevent leakage\n",
    "    X = work.drop(columns=list(set(ALL_TARGET_COLS + [target_col])), errors='ignore')\n",
    "\n",
    "    # Guard against accidental leakage\n",
    "    assert target_col not in X.columns, f\"Target leakage: {target_col} present in predictors!\"\n",
    "\n",
    "    # Drop non-numeric predictors (id is already copied into 'ids')\n",
    "    non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        X = X.drop(columns=non_numeric)\n",
    "\n",
    "    # Sanity: no empty feature set\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(f\"No numeric predictors left after preprocessing for target '{target_col}'.\")\n",
    "\n",
    "    return X, y, ids\n",
    "\n",
    "# ============================================================\n",
    "# RandomizedSearch + LOOCV with per-fold R²\n",
    "# ============================================================\n",
    "def run_target_randomsearch_loocv(target_col: str, units_label: str = \"units\"):\n",
    "    X, y, ids = build_xy(df, target_col)\n",
    "    if X.shape[1] == 0 or len(y) < 3:\n",
    "        print(f\"[ERROR] Not enough predictors or samples for '{target_col}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nTarget: {target_col} | X: {X.shape} | y: {y.shape}\")\n",
    "    # Reset indices to align X, y, ids\n",
    "    y = y.reset_index(drop=True)\n",
    "    X = X.reset_index(drop=True)\n",
    "    ids = ids.reset_index(drop=True)\n",
    "    out_prefix = target_col.replace('.', '_')\n",
    "\n",
    "    # 1. RandomizedSearchCV (global)\n",
    "    print(f\"\\n[{target_col}] Starting global RandomizedSearchCV with {N_ITER_SEARCH} iterations...\")\n",
    "    kfold = KFold(n_splits=min(INNER_FOLDS, len(y)), shuffle=True, random_state=RANDOM_STATE)\n",
    "    base = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS)\n",
    "    tuner = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=RF_PARAM_DIST,\n",
    "        n_iter=N_ITER_SEARCH,\n",
    "        scoring=SCORER,\n",
    "        cv=kfold,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS,\n",
    "        verbose=1,\n",
    "        refit=True,\n",
    "    )\n",
    "    tuner.fit(X, y)\n",
    "\n",
    "    # Leakage sanity check: the tuned model must NOT expect the target as a feature\n",
    "    tuned_features = list(getattr(tuner.best_estimator_, \"feature_names_in_\", []))\n",
    "    if target_col in tuned_features:\n",
    "        raise RuntimeError(\n",
    "            f\"Leakage detected: tuned model expects target '{target_col}' as a feature. \"\n",
    "            f\"Check preprocessing.\"\n",
    "        )\n",
    "\n",
    "    best_params = tuner.best_params_\n",
    "    best_neg_mse = float(tuner.best_score_)\n",
    "    best_rmse = float(np.sqrt(-best_neg_mse))\n",
    "    print(f\"[{target_col}] Best params: {best_params}\")\n",
    "    print(f\"[{target_col}] RandomizedSearch best CV RMSE: {best_rmse:.4f} {units_label}\")\n",
    "    rs_results = pd.DataFrame(tuner.cv_results_)\n",
    "    rs_results.to_csv(\n",
    "        os.path.join(OUT_DIR, f\"{out_prefix}_random_search_results{SUFFIX}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # 2. LOOCV using best params\n",
    "    print(f\"\\n[{target_col}] Starting LOOCV with fixed best hyperparameters...\")\n",
    "    loo = LeaveOneOut()\n",
    "    n = len(y)\n",
    "    y_pred = np.zeros(n, dtype=float)\n",
    "    r2_folds = np.full(n, np.nan, dtype=float)  # one R² per left-out observation\n",
    "    train_means = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(loo.split(X), start=1):\n",
    "        Xtr, Xte = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        ytr = y.iloc[train_idx]\n",
    "        yte = y.iloc[test_idx].values[0]\n",
    "\n",
    "        model = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS, **best_params)\n",
    "        model.fit(Xtr, ytr)\n",
    "        yhat = model.predict(Xte)[0]\n",
    "\n",
    "        # store prediction\n",
    "        test_i = test_idx[0]\n",
    "        y_pred[test_i] = yhat\n",
    "\n",
    "        # per-fold R² for this left-out sample\n",
    "        mu_train = float(ytr.mean())\n",
    "        train_means[test_i] = mu_train\n",
    "        denom = (yte - mu_train)**2\n",
    "        num = (yte - yhat)**2\n",
    "\n",
    "        # if denom == 0, R² is undefined -> keep NaN\n",
    "        if denom != 0.0:\n",
    "            r2_val = 1.0 - (num / denom)\n",
    "            # clamp negative R² to 0\n",
    "            if r2_val < 0.0:\n",
    "                r2_val = 0.0\n",
    "            r2_folds[test_i] = r2_val\n",
    "\n",
    "        if i % 25 == 0 or i == n:\n",
    "            print(f\"  LOOCV progress: {i}/{n}\")\n",
    "\n",
    "    # Global LOOCV metrics over all predictions\n",
    "    rmse_global = mean_squared_error(y, y_pred, squared=False)\n",
    "    r2_raw_global = r2_score(y, y_pred)\n",
    "    # clamp negative global R² to 0 as well\n",
    "    r2_global = max(0.0, r2_raw_global)\n",
    "    print(f\"[{target_col}] LOOCV RMSE (global): {rmse_global:.4f} | \"\n",
    "          f\"R² (global, clamped): {r2_global:.4f} (raw: {r2_raw_global:.4f})\")\n",
    "\n",
    "    # Save per-sample predictions + per-fold R² + id\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"index\": np.arange(n),\n",
    "        \"id\": ids.astype(str),\n",
    "        \"y_obs\": y.values,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"train_mean_y\": train_means,\n",
    "        \"r2_loocv_fold\": r2_folds\n",
    "    })\n",
    "\n",
    "    # CSV for violin plot\n",
    "    violin_csv_path = os.path.join(OUT_DIR, f\"{out_prefix}_violin{SUFFIX}.csv\")\n",
    "    preds_df.to_csv(violin_csv_path, index=False)\n",
    "    print(f\"[{target_col}] Saved per-fold R² CSV: {violin_csv_path}\")\n",
    "\n",
    "    # Also keep the LOOCV predictions CSV\n",
    "    preds_path = os.path.join(OUT_DIR, f\"{out_prefix}_loocv_predictions{SUFFIX}.csv\")\n",
    "    preds_df.to_csv(preds_path, index=False)\n",
    "    print(f\"[{target_col}] Saved LOOCV predictions (with fold R²) to: {preds_path}\")\n",
    "\n",
    "    # Save summary metrics\n",
    "    pd.DataFrame({\n",
    "        \"target\": [target_col],\n",
    "        \"n\": [n],\n",
    "        \"n_predictors\": [X.shape[1]],\n",
    "        \"loocv_rmse_global\": [rmse_global],\n",
    "        \"loocv_r2_global_clamped\": [r2_global],\n",
    "        \"loocv_r2_global_raw\": [r2_raw_global],\n",
    "        \"random_search_best_rmse\": [best_rmse]\n",
    "    }).to_csv(\n",
    "        os.path.join(OUT_DIR, f\"{out_prefix}_loocv_metrics{SUFFIX}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # ----------------- GLOBAL OBS vs PRED PLOT (ALL SAMPLES) -----------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=preds_df[\"y_obs\"], y=preds_df[\"y_pred\"], s=18, edgecolor=None)\n",
    "    lo = float(np.nanmin([preds_df[\"y_obs\"].min(), preds_df[\"y_pred\"].min()]))\n",
    "    hi = float(np.nanmax([preds_df[\"y_obs\"].max(), preds_df[\"y_pred\"].max()]))\n",
    "    plt.plot([lo, hi], [lo, hi], 'k--', lw=2)\n",
    "    plt.xlabel(f\"Observed {target_col}\")\n",
    "    plt.ylabel(f\"Predicted {target_col}\")\n",
    "    plt.title(f\"{target_col}: LOOCV Obs vs Pred (All Samples)\\n\"\n",
    "              f\"RMSE={rmse_global:.3f}, R²={r2_global:.3f} (clamped)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{out_prefix}_loocv_obs_pred{SUFFIX}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ----------------- LCC-ONLY OBS vs PRED PLOT -----------------\n",
    "    # Filter to IDs starting with \"LCC_\"\n",
    "    lcc_mask = preds_df[\"id\"].astype(str).str.startswith(\"LCC_\")\n",
    "    preds_lcc = preds_df[lcc_mask].copy()\n",
    "\n",
    "    if len(preds_lcc) >= 3:\n",
    "        rmse_lcc = mean_squared_error(preds_lcc[\"y_obs\"], preds_lcc[\"y_pred\"], squared=False)\n",
    "        r2_raw_lcc = r2_score(preds_lcc[\"y_obs\"], preds_lcc[\"y_pred\"])\n",
    "        r2_lcc = max(0.0, r2_raw_lcc)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(x=preds_lcc[\"y_obs\"], y=preds_lcc[\"y_pred\"], s=18, edgecolor=None)\n",
    "        lo_lcc = float(np.nanmin([preds_lcc[\"y_obs\"].min(), preds_lcc[\"y_pred\"].min()]))\n",
    "        hi_lcc = float(np.nanmax([preds_lcc[\"y_obs\"].max(), preds_lcc[\"y_pred\"].max()]))\n",
    "\n",
    "        plt.plot([lo_lcc, hi_lcc], [lo_lcc, hi_lcc], 'k--', lw=2)\n",
    "        plt.xlabel(f\"Observed {target_col} (LCC only)\")\n",
    "        plt.ylabel(f\"Predicted {target_col} (LCC only)\")\n",
    "        plt.title(f\"{target_col}: LOOCV Obs vs Pred (LCC Samples)\\n\"\n",
    "                  f\"n={len(preds_lcc)}\")\n",
    "\n",
    "        # Put RMSE and R² in lower-right corner\n",
    "        dx = hi_lcc - lo_lcc\n",
    "        text_x = lo_lcc + 0.98 * dx\n",
    "        text_y = lo_lcc + 0.02 * dx\n",
    "        plt.text(\n",
    "            text_x, text_y,\n",
    "            f\"RMSE={rmse_lcc:.3f}\\nR²={r2_lcc:.3f}\",\n",
    "            ha='right', va='bottom',\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        lcc_png_path = os.path.join(OUT_DIR, f\"{out_prefix}_loocv_obs_pred_LCC_only{SUFFIX}.png\")\n",
    "        plt.savefig(lcc_png_path, dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"[{target_col}] Saved LCC-only Obs vs Pred plot: {lcc_png_path}\")\n",
    "    else:\n",
    "        print(f\"[{target_col}] Not enough LCC samples (found {len(preds_lcc)}) for LCC-only plot.\")\n",
    "\n",
    "    # ----------------- VIOLIN PLOT OF PER-FOLD R² -----------------\n",
    "    valid_r2 = preds_df[\"r2_loocv_fold\"].dropna()\n",
    "    if len(valid_r2) > 0:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.violinplot(y=valid_r2, cut=0)\n",
    "        plt.ylabel(\"Per-fold LOOCV R²\")\n",
    "        plt.title(f\"Distribution of LOOCV R² per left-out sample\\nTarget: {target_col}\")\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        violin_png_path = os.path.join(OUT_DIR, f\"{out_prefix}_violin{SUFFIX}.png\")\n",
    "        plt.savefig(violin_png_path, dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"[{target_col}] Saved violin plot of per-fold R²: {violin_png_path}\")\n",
    "    else:\n",
    "        print(f\"[{target_col}] No valid per-fold R² values (all denominators zero). Skipping violin plot.\")\n",
    "\n",
    "    # ----------------- FINAL MODEL + FEATURE IMPORTANCE -----------------\n",
    "    final_model = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS, **best_params)\n",
    "    final_model.fit(X, y)\n",
    "    model_path = os.path.join(MODEL_DIR, f\"rf_final_{out_prefix}{SUFFIX}.joblib\")\n",
    "    joblib.dump(final_model, model_path)\n",
    "\n",
    "    feature_names = list(getattr(final_model, \"feature_names_in_\", X.columns))\n",
    "\n",
    "    # Feature importance CSV + plot\n",
    "    importances = final_model.feature_importances_\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importances\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    fi_csv_path = os.path.join(OUT_DIR, f\"{out_prefix}_feature_importance{SUFFIX}.csv\")\n",
    "    fi_df.to_csv(fi_csv_path, index=False)\n",
    "    print(f\"[{target_col}] Saved feature importance CSV: {fi_csv_path}\")\n",
    "\n",
    "    top_n = min(30, len(fi_df))\n",
    "    fi_top = fi_df.head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, max(6, 0.3 * top_n)))\n",
    "    sns.barplot(data=fi_top, x=\"importance\", y=\"feature\", orient=\"h\")\n",
    "    plt.xlabel(\"Random Forest feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(f\"Feature importance (top {top_n})\\nTarget: {target_col}\")\n",
    "    plt.tight_layout()\n",
    "    fi_png_path = os.path.join(OUT_DIR, f\"{out_prefix}_feature_importance{SUFFIX}.png\")\n",
    "    plt.savefig(fi_png_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[{target_col}] Saved feature importance plot: {fi_png_path}\")\n",
    "\n",
    "    # Metadata JSON\n",
    "    meta = {\n",
    "        \"target\": target_col,\n",
    "        \"loocv_rmse_global\": rmse_global,\n",
    "        \"loocv_r2_global_clamped\": r2_global,\n",
    "        \"loocv_r2_global_raw\": r2_raw_global,\n",
    "        \"best_params\": best_params,\n",
    "        \"model_path\": model_path,\n",
    "        \"n_samples\": n,\n",
    "        \"feature_names\": feature_names\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, f\"{out_prefix}_final_model_metadata{SUFFIX}.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    # Extra sanity message\n",
    "    print(f\"[{target_col}] Final model trained with {len(meta['feature_names'])} features; \"\n",
    "          f\"'{target_col}' in features? {target_col in meta['feature_names']}\")\n",
    "\n",
    "# ----------------- RUN FOR EACH TARGET -----------------\n",
    "for tcol, units in targets:\n",
    "    run_target_randomsearch_loocv(tcol, units)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d245b-fb95-4ae4-aa08-a96df4ef7906",
   "metadata": {},
   "source": [
    "Predict model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cade604-62cf-4dc8-a98a-ab993376c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features: /explore/nobackup/people/spotter5/new_combustion/2025-10-28_LCC_CombustionModelPredictors.csv\n",
      "Found 1 model(s): ['burn_depth_LCC_11_18']\n",
      "\n",
      "Loading model: /explore/nobackup/people/spotter5/new_combustion/LCC/models/rf_final_burn_depth_LCC_11_18.joblib\n",
      "  Applying model 'burn_depth_LCC_11_18' to 98 rows, 50 features\n",
      "\n",
      "Wrote predictions → /explore/nobackup/people/spotter5/new_combustion/LCC/2025-10-28_LCC_CombustionModelPredictors_predictions_LCC_11_18.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ----------------- PATHS -----------------\n",
    "ROOT_DIR   = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "INPUT_CSV  = os.path.join(ROOT_DIR, \"2025-10-28_LCC_CombustionModelPredictors.csv\")\n",
    "OUT_DIR    = os.path.join(ROOT_DIR, \"LCC\")\n",
    "MODEL_DIR  = os.path.join(OUT_DIR, \"models\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------- MODEL NAMING (MATCH TRAINING) -----------------\n",
    "# We saved models as rf_final_<target><SUFFIX>.joblib, with:\n",
    "SUFFIX        = \"_LCC_11_18\"\n",
    "MODEL_PREFIX  = \"rf_final_\"\n",
    "MODEL_SUFFIX  = f\"{SUFFIX}.joblib\"   # e.g. rf_final_burn_depth_LCC_11_18.joblib\n",
    "\n",
    "# ----------------- EXCLUDED PREDICTOR COLUMNS -----------------\n",
    "EXCLUDE_PRED_COLS = {\n",
    "    'id', 'project.name', 'lat', 'lon', 'burn_year', 'date', 'project',\n",
    "    'ID', 'Id', 'project_name', 'latitude', 'longitude', 'fireYr', 'Date',\n",
    "    'landcover_name'\n",
    "}\n",
    "\n",
    "# ----------------- PREPROCESS (mirror training) -----------------\n",
    "def load_and_clean_dataframe(path: str) -> pd.DataFrame:\n",
    "    print(f\"Reading features: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Standardize column names like in training\n",
    "    rename_map = {}\n",
    "    if 'ID' in df.columns: rename_map['ID'] = 'id'\n",
    "    if 'Id' in df.columns: rename_map['Id'] = 'id'\n",
    "    if 'project_name' in df.columns and 'project.name' not in df.columns:\n",
    "        rename_map['project_name'] = 'project.name'\n",
    "    if 'Date' in df.columns and 'date' not in df.columns:\n",
    "        rename_map['Date'] = 'date'\n",
    "    if 'latitude' in df.columns and 'lat' not in df.columns:\n",
    "        rename_map['latitude'] = 'lat'\n",
    "    if 'longitude' in df.columns and 'lon' not in df.columns:\n",
    "        rename_map['longitude'] = 'lon'\n",
    "    if 'fireYr' in df.columns and 'burn_year' not in df.columns:\n",
    "        rename_map['fireYr'] = 'burn_year'\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # One-hot LandCover like training\n",
    "    if 'LandCover' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['LandCover'],\n",
    "                            prefix='LC', drop_first=True, dummy_na=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_predict_matrix(df_in: pd.DataFrame,\n",
    "                         model_feature_names: List[str],\n",
    "                         target_cols_to_drop: List[str]) -> pd.DataFrame:\n",
    "    # Drop non-predictor columns\n",
    "    drop_cols = [c for c in EXCLUDE_PRED_COLS if c in df_in.columns]\n",
    "    X = df_in.drop(columns=drop_cols, errors='ignore').copy()\n",
    "\n",
    "    # Drop any known target cols (if they happen to be present)\n",
    "    X = X.drop(columns=[c for c in target_cols_to_drop if c in X.columns], errors='ignore')\n",
    "\n",
    "    # Keep only numeric columns\n",
    "    non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        X = X.drop(columns=non_numeric)\n",
    "\n",
    "    # Align to model features:\n",
    "    # 1) add any missing columns as 0\n",
    "    missing = [c for c in model_feature_names if c not in X.columns]\n",
    "    if missing:\n",
    "        for c in missing:\n",
    "            X[c] = 0.0\n",
    "\n",
    "    # 2) drop any extras and order exactly like model\n",
    "    X = X[model_feature_names]\n",
    "\n",
    "    return X\n",
    "\n",
    "def find_models(model_dir: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Find rf_final_* models that end with our SUFFIX (e.g. rf_final_burn_depth_LCC_11_18.joblib).\n",
    "    Returns list of (target_name, full_path).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    if not os.path.isdir(model_dir):\n",
    "        print(f\"[WARN] Model directory not found: {model_dir}\")\n",
    "        return out\n",
    "\n",
    "    for fn in os.listdir(model_dir):\n",
    "        if fn.startswith(MODEL_PREFIX) and fn.endswith(MODEL_SUFFIX):\n",
    "            path = os.path.join(model_dir, fn)\n",
    "            # infer target name from filename, e.g. rf_final_burn_depth_LCC_11_18.joblib\n",
    "            # target_name = \"burn_depth_LCC_11_18\" here\n",
    "            target = fn[len(MODEL_PREFIX):-len(\".joblib\")]\n",
    "            out.append((target, path))\n",
    "    out.sort()\n",
    "    return out\n",
    "\n",
    "# ----------------- MAIN -----------------\n",
    "def main():\n",
    "    df = load_and_clean_dataframe(INPUT_CSV)\n",
    "\n",
    "    # If your training used these target columns, list them so we drop them from predictors:\n",
    "    candidate_targets = ['combusted_above', 'above.carbon.combusted',\n",
    "                         'combusted_below', 'burn_depth']\n",
    "    present_targets = [c for c in candidate_targets if c in df.columns]\n",
    "\n",
    "    models = find_models(MODEL_DIR)\n",
    "    if not models:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No models found in {MODEL_DIR} matching {MODEL_PREFIX}*{MODEL_SUFFIX}\"\n",
    "        )\n",
    "\n",
    "    print(f\"Found {len(models)} model(s): {[t for t, _ in models]}\")\n",
    "\n",
    "    preds = {}   # target_name -> prediction vector\n",
    "\n",
    "    for target_name, model_path in models:\n",
    "        print(f\"\\nLoading model: {model_path}\")\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "        # Feature alignment: use model.feature_names_in_\n",
    "        if hasattr(model, \"feature_names_in_\"):\n",
    "            feature_names = list(model.feature_names_in_)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Model {model_path} is missing feature_names_in_. \"\n",
    "                f\"Retrain with scikit-learn >=1.0 so this attribute is saved.\"\n",
    "            )\n",
    "\n",
    "        X = build_predict_matrix(df, feature_names, present_targets)\n",
    "\n",
    "        print(f\"  Applying model '{target_name}' to {len(X)} rows, {X.shape[1]} features\")\n",
    "        y_pred = model.predict(X)\n",
    "        preds[f\"pred_{target_name}\"] = y_pred\n",
    "\n",
    "    # Assemble output table (keep some identifiers if present)\n",
    "    keep_cols = []\n",
    "    for c in [\"id\", \"project.name\", \"lat\", \"lon\", \"burn_year\", \"date\"]:\n",
    "        if c in df.columns:\n",
    "            keep_cols.append(c)\n",
    "    out_df = df[keep_cols].copy() if keep_cols else pd.DataFrame(index=df.index)\n",
    "\n",
    "    for col, arr in preds.items():\n",
    "        out_df[col] = arr\n",
    "\n",
    "    # Write predictions\n",
    "    base = os.path.splitext(os.path.basename(INPUT_CSV))[0]\n",
    "    out_csv = os.path.join(OUT_DIR, f\"{base}_predictions{SUFFIX}.csv\")\n",
    "    out_df.to_csv(out_csv, index=False)\n",
    "    print(f\"\\nWrote predictions → {out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71face75-9846-4232-8a36-e298b7e13155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
